{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.3\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import detectron2\n",
    "\n",
    "# Get PyTorch version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "print(\"torch: \", TORCH_VERSION)\n",
    "\n",
    "# Get Detectron2 version\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {1: 1.4357638888888888, 2: 0.7671614100185529}\n",
      "Class weights calculated successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Load your JSON file\n",
    "with open('net_data/train_dataset/train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize counters for each class\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# Step 2: Count instances of each category\n",
    "for instance in data['annotations']:\n",
    "    category_id = instance['category_id']\n",
    "    class_counts[category_id] += 1\n",
    "\n",
    "# Calculate total number of instances\n",
    "total_instances = sum(class_counts.values())\n",
    "\n",
    "# Step 3: Calculate class weights\n",
    "class_weights = {category_id: total_instances / (len(class_counts) * count) for category_id, count in class_counts.items()}\n",
    "\n",
    "# Print the class weights\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# class weights are saved in json file \n",
    "with open('class_weights.json', 'w') as f:\n",
    "    json.dump(class_weights, f, indent=2)\n",
    "\n",
    "print(\"Class weights calculated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 16:52:51 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[06/27 16:52:51 d2.data.datasets.coco]: \u001b[0mLoaded 228 images in COCO format from net_data/train_dataset/train.json\n",
      "\u001b[32m[06/27 16:52:51 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 228 images left.\n",
      "\u001b[32m[06/27 16:52:51 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|    NETs    | 1728         |  non-net   | 3234         |\n",
      "|            |              |            |              |\n",
      "|   total    | 4962         |            |              |\u001b[0m\n",
      "\u001b[32m[06/27 16:52:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[06/27 16:52:51 d2.data.build]: \u001b[0mUsing training sampler RepeatFactorTrainingSampler\n",
      "\u001b[32m[06/27 16:52:51 d2.data.samplers.distributed_sampler]: \u001b[0mCat ID 0: freq=0.86, rep=1.53\n",
      "\u001b[32m[06/27 16:52:51 d2.data.samplers.distributed_sampler]: \u001b[0mCat ID 1: freq=0.91, rep=1.48\n",
      "\u001b[32m[06/27 16:52:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[06/27 16:52:51 d2.data.common]: \u001b[0mSerializing 228 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/27 16:52:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.98 MiB\n",
      "\u001b[32m[06/27 16:52:51 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[32m[06/27 16:52:51 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 16:52:51 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dk/Desktop/complete_detectron_/.detectron2-env/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 16:54:26 d2.utils.events]: \u001b[0m eta: 1:13:17  iter: 19  total_loss: 2.575  loss_cls: 1.115  loss_box_reg: 0.7503  loss_rpn_cls: 0.5218  loss_rpn_loc: 0.1215    time: 4.5780  last_time: 5.7941  data_time: 0.0687  last_data_time: 0.0011   lr: 4.9953e-06  \n",
      "\u001b[32m[06/27 16:57:08 d2.utils.events]: \u001b[0m eta: 1:36:11  iter: 39  total_loss: 2.409  loss_cls: 1.045  loss_box_reg: 0.7938  loss_rpn_cls: 0.4502  loss_rpn_loc: 0.1286    time: 6.4047  last_time: 5.8792  data_time: 0.0027  last_data_time: 0.0033   lr: 9.9902e-06  \n",
      "\u001b[32m[06/27 17:00:02 d2.utils.events]: \u001b[0m eta: 1:57:51  iter: 59  total_loss: 2.075  loss_cls: 0.9275  loss_box_reg: 0.8226  loss_rpn_cls: 0.1952  loss_rpn_loc: 0.089    time: 7.1971  last_time: 10.1143  data_time: 0.0036  last_data_time: 0.0024   lr: 1.4985e-05  \n",
      "\u001b[32m[06/27 17:02:51 d2.utils.events]: \u001b[0m eta: 1:58:38  iter: 79  total_loss: 1.765  loss_cls: 0.8022  loss_box_reg: 0.7658  loss_rpn_cls: 0.06735  loss_rpn_loc: 0.08514    time: 7.5187  last_time: 8.7926  data_time: 0.0042  last_data_time: 0.0051   lr: 1.998e-05  \n",
      "\u001b[32m[06/27 17:05:33 d2.utils.events]: \u001b[0m eta: 2:00:23  iter: 99  total_loss: 1.75  loss_cls: 0.723  loss_box_reg: 0.8246  loss_rpn_cls: 0.07417  loss_rpn_loc: 0.1104    time: 7.6369  last_time: 8.3701  data_time: 0.0037  last_data_time: 0.0050   lr: 2.4975e-05  \n",
      "\u001b[32m[06/27 17:08:18 d2.utils.events]: \u001b[0m eta: 1:58:57  iter: 119  total_loss: 1.667  loss_cls: 0.6701  loss_box_reg: 0.786  loss_rpn_cls: 0.0811  loss_rpn_loc: 0.09355    time: 7.7431  last_time: 8.1075  data_time: 0.0041  last_data_time: 0.0048   lr: 2.997e-05  \n",
      "\u001b[32m[06/27 17:10:58 d2.utils.events]: \u001b[0m eta: 1:56:14  iter: 139  total_loss: 1.617  loss_cls: 0.6409  loss_box_reg: 0.8302  loss_rpn_cls: 0.05176  loss_rpn_loc: 0.08952    time: 7.7773  last_time: 8.3470  data_time: 0.0030  last_data_time: 0.0033   lr: 3.4965e-05  \n",
      "\u001b[32m[06/27 17:28:25 d2.utils.events]: \u001b[0m eta: 1:52:49  iter: 159  total_loss: 1.609  loss_cls: 0.6251  loss_box_reg: 0.8204  loss_rpn_cls: 0.06789  loss_rpn_loc: 0.09547    time: 7.6648  last_time: 5.3986  data_time: 0.0028  last_data_time: 0.0014   lr: 3.996e-05  \n",
      "\u001b[32m[06/27 17:31:53 d2.utils.events]: \u001b[0m eta: 1:50:08  iter: 179  total_loss: 1.555  loss_cls: 0.5939  loss_box_reg: 0.8238  loss_rpn_cls: 0.06468  loss_rpn_loc: 0.09145    time: 7.9722  last_time: 14.1258  data_time: 0.0033  last_data_time: 0.0044   lr: 4.4955e-05  \n",
      "\u001b[32m[06/27 17:35:37 d2.utils.events]: \u001b[0m eta: 1:48:32  iter: 199  total_loss: 1.56  loss_cls: 0.5934  loss_box_reg: 0.802  loss_rpn_cls: 0.05448  loss_rpn_loc: 0.08688    time: 8.2986  last_time: 10.1838  data_time: 0.0033  last_data_time: 0.0066   lr: 4.995e-05  \n",
      "\u001b[32m[06/27 17:38:52 d2.utils.events]: \u001b[0m eta: 1:47:03  iter: 219  total_loss: 1.479  loss_cls: 0.5542  loss_box_reg: 0.7576  loss_rpn_cls: 0.06675  loss_rpn_loc: 0.1058    time: 8.4306  last_time: 7.9738  data_time: 0.0031  last_data_time: 0.0029   lr: 5.4945e-05  \n",
      "\u001b[32m[06/27 17:41:39 d2.utils.events]: \u001b[0m eta: 1:44:32  iter: 239  total_loss: 1.457  loss_cls: 0.5208  loss_box_reg: 0.7642  loss_rpn_cls: 0.0604  loss_rpn_loc: 0.09083    time: 8.4263  last_time: 4.0149  data_time: 0.0023  last_data_time: 0.0009   lr: 5.994e-05  \n",
      "\u001b[32m[06/27 17:43:31 d2.utils.events]: \u001b[0m eta: 1:40:04  iter: 259  total_loss: 1.435  loss_cls: 0.529  loss_box_reg: 0.7777  loss_rpn_cls: 0.05956  loss_rpn_loc: 0.07961    time: 8.2043  last_time: 8.6078  data_time: 0.0021  last_data_time: 0.0027   lr: 6.4935e-05  \n",
      "\u001b[32m[06/27 17:45:57 d2.utils.events]: \u001b[0m eta: 1:36:58  iter: 279  total_loss: 1.353  loss_cls: 0.5034  loss_box_reg: 0.7324  loss_rpn_cls: 0.04642  loss_rpn_loc: 0.08634    time: 8.1409  last_time: 6.9946  data_time: 0.0032  last_data_time: 0.0014   lr: 6.993e-05  \n",
      "\u001b[32m[06/27 17:48:24 d2.utils.events]: \u001b[0m eta: 1:33:01  iter: 299  total_loss: 1.319  loss_cls: 0.4763  loss_box_reg: 0.754  loss_rpn_cls: 0.04754  loss_rpn_loc: 0.0704    time: 8.0864  last_time: 9.6882  data_time: 0.0028  last_data_time: 0.0038   lr: 7.4925e-05  \n",
      "\u001b[32m[06/27 17:51:01 d2.utils.events]: \u001b[0m eta: 1:30:19  iter: 319  total_loss: 1.37  loss_cls: 0.4767  loss_box_reg: 0.73  loss_rpn_cls: 0.05675  loss_rpn_loc: 0.0858    time: 8.0708  last_time: 9.1106  data_time: 0.0037  last_data_time: 0.0018   lr: 7.992e-05  \n",
      "\u001b[32m[06/27 17:53:28 d2.utils.events]: \u001b[0m eta: 1:26:24  iter: 339  total_loss: 1.287  loss_cls: 0.4374  loss_box_reg: 0.715  loss_rpn_cls: 0.05126  loss_rpn_loc: 0.07697    time: 8.0295  last_time: 7.8126  data_time: 0.0031  last_data_time: 0.0021   lr: 8.4915e-05  \n",
      "\u001b[32m[06/27 17:55:58 d2.utils.events]: \u001b[0m eta: 1:23:27  iter: 359  total_loss: 1.211  loss_cls: 0.4185  loss_box_reg: 0.6786  loss_rpn_cls: 0.04108  loss_rpn_loc: 0.07767    time: 8.0007  last_time: 6.7864  data_time: 0.0030  last_data_time: 0.0033   lr: 8.991e-05  \n",
      "\u001b[32m[06/27 17:58:14 d2.utils.events]: \u001b[0m eta: 1:19:36  iter: 379  total_loss: 1.073  loss_cls: 0.3322  loss_box_reg: 0.595  loss_rpn_cls: 0.04432  loss_rpn_loc: 0.1097    time: 7.9363  last_time: 6.4355  data_time: 0.0023  last_data_time: 0.0026   lr: 9.4905e-05  \n",
      "\u001b[32m[06/27 18:00:39 d2.utils.events]: \u001b[0m eta: 1:16:50  iter: 399  total_loss: 1.123  loss_cls: 0.379  loss_box_reg: 0.5884  loss_rpn_cls: 0.04555  loss_rpn_loc: 0.07752    time: 7.9007  last_time: 6.0896  data_time: 0.0025  last_data_time: 0.0023   lr: 9.99e-05  \n",
      "\u001b[32m[06/27 18:03:03 d2.utils.events]: \u001b[0m eta: 1:14:18  iter: 419  total_loss: 0.9954  loss_cls: 0.3361  loss_box_reg: 0.5084  loss_rpn_cls: 0.03605  loss_rpn_loc: 0.07676    time: 7.8679  last_time: 5.6428  data_time: 0.0024  last_data_time: 0.0022   lr: 0.0001049  \n",
      "\u001b[32m[06/27 18:05:33 d2.utils.events]: \u001b[0m eta: 1:11:42  iter: 439  total_loss: 1.033  loss_cls: 0.3486  loss_box_reg: 0.4935  loss_rpn_cls: 0.0506  loss_rpn_loc: 0.08689    time: 7.8503  last_time: 7.4969  data_time: 0.0026  last_data_time: 0.0014   lr: 0.00010989  \n",
      "\u001b[32m[06/27 18:08:01 d2.utils.events]: \u001b[0m eta: 1:09:10  iter: 459  total_loss: 0.8528  loss_cls: 0.3169  loss_box_reg: 0.432  loss_rpn_cls: 0.03281  loss_rpn_loc: 0.08376    time: 7.8314  last_time: 7.7720  data_time: 0.0026  last_data_time: 0.0014   lr: 0.00011489  \n",
      "\u001b[32m[06/27 18:25:18 d2.utils.events]: \u001b[0m eta: 1:06:17  iter: 479  total_loss: 0.979  loss_cls: 0.35  loss_box_reg: 0.519  loss_rpn_cls: 0.0418  loss_rpn_loc: 0.07943    time: 7.7729  last_time: 10.6430  data_time: 0.0020  last_data_time: 0.0055   lr: 0.00011988  \n",
      "\u001b[32m[06/27 18:39:20 d2.utils.events]: \u001b[0m eta: 1:03:08  iter: 499  total_loss: 0.8299  loss_cls: 0.3049  loss_box_reg: 0.4316  loss_rpn_cls: 0.03784  loss_rpn_loc: 0.07106    time: 7.6916  last_time: 5.2158  data_time: 0.0031  last_data_time: 0.0030   lr: 0.00012488  \n",
      "\u001b[32m[06/27 18:41:02 d2.utils.events]: \u001b[0m eta: 1:00:00  iter: 519  total_loss: 0.8365  loss_cls: 0.276  loss_box_reg: 0.3975  loss_rpn_cls: 0.05532  loss_rpn_loc: 0.08232    time: 7.5923  last_time: 5.9673  data_time: 0.0021  last_data_time: 0.0016   lr: 0.00012987  \n",
      "\u001b[32m[06/27 18:43:11 d2.utils.events]: \u001b[0m eta: 0:56:42  iter: 539  total_loss: 0.7042  loss_cls: 0.2378  loss_box_reg: 0.3436  loss_rpn_cls: 0.03638  loss_rpn_loc: 0.08876    time: 7.5488  last_time: 5.7698  data_time: 0.0031  last_data_time: 0.0020   lr: 0.00013487  \n",
      "\u001b[32m[06/27 18:45:18 d2.utils.events]: \u001b[0m eta: 0:53:45  iter: 559  total_loss: 0.767  loss_cls: 0.2884  loss_box_reg: 0.375  loss_rpn_cls: 0.02848  loss_rpn_loc: 0.07478    time: 7.5067  last_time: 6.1631  data_time: 0.0028  last_data_time: 0.0040   lr: 0.00013986  \n",
      "\u001b[32m[06/27 18:47:27 d2.utils.events]: \u001b[0m eta: 0:50:47  iter: 579  total_loss: 0.7497  loss_cls: 0.2759  loss_box_reg: 0.3401  loss_rpn_cls: 0.02837  loss_rpn_loc: 0.07916    time: 7.4704  last_time: 6.7908  data_time: 0.0031  last_data_time: 0.0013   lr: 0.00014486  \n",
      "\u001b[32m[06/27 18:49:37 d2.utils.events]: \u001b[0m eta: 0:48:00  iter: 599  total_loss: 0.8616  loss_cls: 0.2984  loss_box_reg: 0.3901  loss_rpn_cls: 0.04539  loss_rpn_loc: 0.07386    time: 7.4368  last_time: 6.4398  data_time: 0.0026  last_data_time: 0.0013   lr: 0.00014985  \n",
      "\u001b[32m[06/27 18:51:44 d2.utils.events]: \u001b[0m eta: 0:45:19  iter: 619  total_loss: 0.8128  loss_cls: 0.3014  loss_box_reg: 0.3778  loss_rpn_cls: 0.03607  loss_rpn_loc: 0.09001    time: 7.4026  last_time: 7.8863  data_time: 0.0023  last_data_time: 0.0013   lr: 0.00015485  \n",
      "\u001b[32m[06/27 18:53:42 d2.utils.events]: \u001b[0m eta: 0:42:30  iter: 639  total_loss: 0.6566  loss_cls: 0.2462  loss_box_reg: 0.329  loss_rpn_cls: 0.03355  loss_rpn_loc: 0.07179    time: 7.3555  last_time: 5.3699  data_time: 0.0022  last_data_time: 0.0025   lr: 0.00015984  \n",
      "\u001b[32m[06/27 18:55:41 d2.utils.events]: \u001b[0m eta: 0:39:43  iter: 659  total_loss: 0.6991  loss_cls: 0.2521  loss_box_reg: 0.3357  loss_rpn_cls: 0.02245  loss_rpn_loc: 0.06071    time: 7.3120  last_time: 5.9386  data_time: 0.0020  last_data_time: 0.0018   lr: 0.00016484  \n",
      "\u001b[32m[06/27 18:57:42 d2.utils.events]: \u001b[0m eta: 0:37:08  iter: 679  total_loss: 0.8167  loss_cls: 0.2974  loss_box_reg: 0.3747  loss_rpn_cls: 0.03164  loss_rpn_loc: 0.08048    time: 7.2752  last_time: 6.6481  data_time: 0.0019  last_data_time: 0.0014   lr: 0.00016983  \n",
      "\u001b[32m[06/27 19:00:00 d2.utils.events]: \u001b[0m eta: 0:34:50  iter: 699  total_loss: 0.7188  loss_cls: 0.2518  loss_box_reg: 0.3682  loss_rpn_cls: 0.02785  loss_rpn_loc: 0.08276    time: 7.2639  last_time: 7.7975  data_time: 0.0023  last_data_time: 0.0038   lr: 0.00017483  \n",
      "\u001b[32m[06/27 19:17:14 d2.utils.events]: \u001b[0m eta: 0:32:31  iter: 719  total_loss: 0.664  loss_cls: 0.2474  loss_box_reg: 0.35  loss_rpn_cls: 0.03235  loss_rpn_loc: 0.07131    time: 7.2512  last_time: 5.5793  data_time: 0.0026  last_data_time: 0.0022   lr: 0.00017982  \n",
      "\u001b[32m[06/27 19:51:14 d2.utils.events]: \u001b[0m eta: 0:29:52  iter: 739  total_loss: 0.759  loss_cls: 0.2775  loss_box_reg: 0.3568  loss_rpn_cls: 0.03592  loss_rpn_loc: 0.06321    time: 7.1891  last_time: 4.5473  data_time: 0.0014  last_data_time: 0.0018   lr: 0.00018482  \n",
      "\u001b[32m[06/27 19:52:52 d2.utils.events]: \u001b[0m eta: 0:27:13  iter: 759  total_loss: 0.7386  loss_cls: 0.2855  loss_box_reg: 0.3527  loss_rpn_cls: 0.03951  loss_rpn_loc: 0.07144    time: 7.1285  last_time: 4.0466  data_time: 0.0016  last_data_time: 0.0016   lr: 0.00018981  \n",
      "\u001b[32m[06/27 19:57:35 d2.utils.events]: \u001b[0m eta: 0:24:50  iter: 779  total_loss: 0.6643  loss_cls: 0.2604  loss_box_reg: 0.3494  loss_rpn_cls: 0.02612  loss_rpn_loc: 0.0668    time: 7.0832  last_time: 5.6402  data_time: 0.0024  last_data_time: 0.0010   lr: 0.00019481  \n",
      "\u001b[32m[06/27 19:59:14 d2.utils.events]: \u001b[0m eta: 0:22:28  iter: 799  total_loss: 0.7264  loss_cls: 0.266  loss_box_reg: 0.357  loss_rpn_cls: 0.02848  loss_rpn_loc: 0.06404    time: 7.0300  last_time: 5.4070  data_time: 0.0021  last_data_time: 0.0022   lr: 0.0001998  \n",
      "\u001b[32m[06/27 20:01:18 d2.utils.events]: \u001b[0m eta: 0:20:09  iter: 819  total_loss: 0.7075  loss_cls: 0.2623  loss_box_reg: 0.3515  loss_rpn_cls: 0.03089  loss_rpn_loc: 0.07074    time: 7.0092  last_time: 5.9667  data_time: 0.0030  last_data_time: 0.0016   lr: 0.0002048  \n",
      "\u001b[32m[06/27 20:03:17 d2.utils.events]: \u001b[0m eta: 0:17:53  iter: 839  total_loss: 0.7078  loss_cls: 0.2383  loss_box_reg: 0.3264  loss_rpn_cls: 0.02819  loss_rpn_loc: 0.07262    time: 6.9834  last_time: 7.5403  data_time: 0.0020  last_data_time: 0.0013   lr: 0.00020979  \n",
      "\u001b[32m[06/27 20:05:15 d2.utils.events]: \u001b[0m eta: 0:15:36  iter: 859  total_loss: 0.6751  loss_cls: 0.238  loss_box_reg: 0.2961  loss_rpn_cls: 0.0323  loss_rpn_loc: 0.0828    time: 6.9582  last_time: 6.6232  data_time: 0.0022  last_data_time: 0.0034   lr: 0.00021479  \n",
      "\u001b[32m[06/27 20:07:22 d2.utils.events]: \u001b[0m eta: 0:13:22  iter: 879  total_loss: 0.7245  loss_cls: 0.2546  loss_box_reg: 0.3493  loss_rpn_cls: 0.02815  loss_rpn_loc: 0.07184    time: 6.9445  last_time: 7.1761  data_time: 0.0029  last_data_time: 0.0024   lr: 0.00021978  \n",
      "\u001b[32m[06/27 20:09:28 d2.utils.events]: \u001b[0m eta: 0:11:08  iter: 899  total_loss: 0.63  loss_cls: 0.2199  loss_box_reg: 0.3017  loss_rpn_cls: 0.0271  loss_rpn_loc: 0.05529    time: 6.9302  last_time: 5.8518  data_time: 0.0026  last_data_time: 0.0022   lr: 0.00022478  \n",
      "\u001b[32m[06/27 20:11:25 d2.utils.events]: \u001b[0m eta: 0:08:52  iter: 919  total_loss: 0.6241  loss_cls: 0.2306  loss_box_reg: 0.3261  loss_rpn_cls: 0.02308  loss_rpn_loc: 0.07371    time: 6.9067  last_time: 6.4381  data_time: 0.0024  last_data_time: 0.0016   lr: 0.00022977  \n",
      "\u001b[32m[06/27 20:13:22 d2.utils.events]: \u001b[0m eta: 0:06:38  iter: 939  total_loss: 0.667  loss_cls: 0.2246  loss_box_reg: 0.2988  loss_rpn_cls: 0.02395  loss_rpn_loc: 0.07151    time: 6.8841  last_time: 5.6217  data_time: 0.0024  last_data_time: 0.0014   lr: 0.00023477  \n",
      "\u001b[32m[06/27 20:15:30 d2.utils.events]: \u001b[0m eta: 0:04:25  iter: 959  total_loss: 0.7276  loss_cls: 0.2493  loss_box_reg: 0.3478  loss_rpn_cls: 0.0402  loss_rpn_loc: 0.08258    time: 6.8739  last_time: 6.5310  data_time: 0.0022  last_data_time: 0.0023   lr: 0.00023976  \n",
      "\u001b[32m[06/27 20:17:34 d2.utils.events]: \u001b[0m eta: 0:02:12  iter: 979  total_loss: 0.7123  loss_cls: 0.2573  loss_box_reg: 0.3493  loss_rpn_cls: 0.0353  loss_rpn_loc: 0.06923    time: 6.8602  last_time: 4.4573  data_time: 0.0026  last_data_time: 0.0014   lr: 0.00024476  \n",
      "\u001b[32m[06/27 20:19:46 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 999  total_loss: 0.6275  loss_cls: 0.2297  loss_box_reg: 0.3159  loss_rpn_cls: 0.02306  loss_rpn_loc: 0.07997    time: 6.8546  last_time: 7.4157  data_time: 0.0028  last_data_time: 0.0038   lr: 0.00024975  \n",
      "\u001b[32m[06/27 20:19:46 d2.engine.hooks]: \u001b[0mOverall training speed: 998 iterations in 1:54:00 (6.8546 s / it)\n",
      "\u001b[32m[06/27 20:19:46 d2.engine.hooks]: \u001b[0mTotal training time: 1:54:02 (0:00:01 on hooks)\n",
      "\u001b[32m[06/27 20:19:46 d2.data.datasets.coco]: \u001b[0mLoaded 22 images in COCO format from net_data/validation_dataset/val.json\n",
      "\u001b[32m[06/27 20:19:46 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|    NETs    | 152          |  non-net   | 379          |\n",
      "|            |              |            |              |\n",
      "|   total    | 531          |            |              |\u001b[0m\n",
      "\u001b[32m[06/27 20:19:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/27 20:19:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[06/27 20:19:46 d2.data.common]: \u001b[0mSerializing 22 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/27 20:19:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.10 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/27 20:19:46 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[06/27 20:19:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 22 batches\n",
      "\u001b[32m[06/27 20:20:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/22. Dataloading: 0.0004 s/iter. Inference: 1.3880 s/iter. Eval: 0.0002 s/iter. Total: 1.3886 s/iter. ETA=0:00:15\n",
      "\u001b[32m[06/27 20:20:09 d2.evaluation.evaluator]: \u001b[0mInference done 15/22. Dataloading: 0.0005 s/iter. Inference: 1.3633 s/iter. Eval: 0.0002 s/iter. Total: 1.3643 s/iter. ETA=0:00:09\n",
      "\u001b[32m[06/27 20:20:14 d2.evaluation.evaluator]: \u001b[0mInference done 19/22. Dataloading: 0.0005 s/iter. Inference: 1.3493 s/iter. Eval: 0.0002 s/iter. Total: 1.3502 s/iter. ETA=0:00:04\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:23.188557 (1.364033 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:22 (1.333542 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to net_data/output_results/coco_instances_results.json\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.599\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.818\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.695\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.392\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.668\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.230\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.058\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.721\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.764\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.507\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 59.911 | 81.763 | 69.510 | 39.212 | 66.784 | 23.042 |\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|\n",
      "| NETs       | 47.922 | non-net    | 71.900 |\n",
      "\u001b[32m[06/27 20:20:18 d2.engine.defaults]: \u001b[0mEvaluation results for my_dataset_val in csv format:\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[06/27 20:20:18 d2.evaluation.testing]: \u001b[0mcopypaste: 59.9109,81.7627,69.5096,39.2121,66.7838,23.0416\n",
      "\u001b[32m[06/27 20:20:19 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from net_data/output_results/model_final.pth ...\n",
      "\u001b[32m[06/27 20:20:19 d2.data.datasets.coco]: \u001b[0mLoaded 22 images in COCO format from net_data/validation_dataset/val.json\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr-3-11-21NET488_MPO633_014-tif_part_1_png.rf.d0120a5e3f3916f48dc79d173ff703f1.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr-3-11-21NET488_MPO633_021-tif_part_2_png.rf.ddc235d9ac25e83865f904bad9312015.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr_27-10-21NET488_MPO-633_007-tif_part_1_png.rf.d7bb0a0bec99f66852f6f64fd352ef04.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA_27-10-21NET488_MPO-633_009-tif_part_5_png.rf.bdb46095193cde64afc3a81cd4a80c93.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr_27-10-21NET488_MPO-633_004-tif_part_6_png.rf.b24b14e83c5f96320f11f87e0c96e8b9.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr-3-11-21NET488_MPO633_017-tif_part_2_png.rf.bffd4c06e57ec93fef304024d5f54aa8.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA-3-11-21NET488_MPO633_009-tif_part_3_png.rf.b560800a716b2f6e8e3c8cde8c48d9eb.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr_27-10-21NET488_MPO-633_010-tif_part_2_png.rf.a318fd3cc1e508795c643da9df557506.jpg\n",
      "Saved net_data/output_results/predictions/PMA_2h_DFO-Zeitkinetik_4-tif_part_3_png.rf.a9f446497d7503f4e3f244b788da8f18.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr-3-11-21NET488_MPO633_021-tif_part_6_png.rf.884c6ee7e2f165e18ec0567a9ffa9baf.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA-3-11-21NET488_MPO633_006-tif_part_4_png.rf.69fdc4c397604e4b33ab7dd9e0517af0.jpg\n",
      "Saved net_data/output_results/predictions/DFO_3h_DFO-Zeitkinetik_6-tif_part_4_png.rf.235bcdb630d1635371e7a26fda8e648b.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr_27-10-21NET488_MPO-633_010-tif_part_1_png.rf.2f3a9c23c022ee94b54d0c930e028766.jpg\n",
      "Saved net_data/output_results/predictions/DFO-PMA_4h_DFO-Zeitkinetik_6-tif_part_4_png.rf.1f122ed78f4ba10c64119b64a98f7eba.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA-3-11-21NET488_MPO633_012-tif_part_1_png.rf.4290a7a14d8fe2e85006159f28e6a02e.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA_27-10-21NET488_MPO-633_005-tif_part_5_png.rf.275ee5bace618c7e4abcf59d57c8af66.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr_27-10-21NET488_MPO-633_004-tif_part_3_png.rf.39b3f4ec86f76981ba73513961760616.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_ctr-3-11-21NET488_MPO633_014-tif_part_4_png.rf.7f1957838401b5ace08dca26518e8830.jpg\n",
      "Saved net_data/output_results/predictions/DFO_3h_DFO-Zeitkinetik_5-tif_part_1_png.rf.800dc4c85d0fa509318f4ae6cd82c5d7.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA-3-11-21NET488_MPO633_006-tif_part_3_png.rf.170bdef2dd24dabcfaaa1d23df72fe13.jpg\n",
      "Saved net_data/output_results/predictions/20211111-lif_PMA_27-10-21NET488_MPO-633_009-tif_part_4_png.rf.6ddce669c079472a9c0ad475c84847d7.jpg\n",
      "Saved net_data/output_results/predictions/DFO_3h_DFO-Zeitkinetik_6-tif_part_6_png.rf.1be5c192b0d6bc0f2753a2c6bee96caf.jpg\n",
      "Predictions saved to net_data/output_results/predictions/predictions.csv\n",
      "Counts per category per file saved to net_data/output_results/predictions/counts_per_category_per_file.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import cv2\n",
    "\n",
    "# Register your dataset\n",
    "register_coco_instances(\"dataset_dicts\", {}, \n",
    "    \"net_data/train_dataset/train.json\", \n",
    "    \"net_data/train_dataset\")\n",
    "register_coco_instances(\"my_dataset_val\", {}, \n",
    "    \"net_data/validation_dataset/val.json\", \n",
    "    \"net_data/validation_dataset\")\n",
    "\n",
    "# Define class weights\n",
    "class_weights = {0: 1.4357638888888888, 1: 0.7671614100185529}\n",
    "\n",
    "# Setup the configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"dataset_dicts\",)\n",
    "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 1000  # Increase the number of iterations for better training\n",
    "cfg.SOLVER.STEPS = []\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_weights)  # Update with the number of classes\n",
    "\n",
    "# Data loader settings for class balancing\n",
    "cfg.DATALOADER.SAMPLER_TRAIN = \"RepeatFactorTrainingSampler\"\n",
    "cfg.DATALOADER.REPEAT_THRESHOLD = 2.0  # Ensure this is a float\n",
    "\n",
    "# Use CPU\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "# Output directory, if you are retraining model delete the content of this folder before training\n",
    "cfg.OUTPUT_DIR = \"net_data/output_results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the configuration to a yaml file\n",
    "config_yaml_path = os.path.join(cfg.OUTPUT_DIR, \"config.yaml\")\n",
    "with open(config_yaml_path, 'w') as file:\n",
    "    file.write(cfg.dump())\n",
    "\n",
    "# Define a custom trainer to add evaluator\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "# Train the model\n",
    "trainer = CocoTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# Prediction\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import cv2\n",
    "\n",
    "# Load the configuration and trained model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(config_yaml_path)  # Path to your saved config.yaml file\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Path to your trained model weights\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"my_dataset_val\", )\n",
    "\n",
    "# Use CPU for prediction\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Predict for the entire validation dataset\n",
    "val_dataset_dicts = DatasetCatalog.get(\"my_dataset_val\")\n",
    "metadata = MetadataCatalog.get(\"my_dataset_val\")\n",
    "\n",
    "output_results_dir = os.path.join(cfg.OUTPUT_DIR, \"predictions\")\n",
    "os.makedirs(output_results_dir, exist_ok=True)\n",
    "\n",
    "# Prepare CSV file\n",
    "import csv\n",
    "csv_file_path = os.path.join(output_results_dir, \"predictions.csv\")\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['file_name', 'class_id', 'class_name', 'score', 'bbox']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for d in val_dataset_dicts:\n",
    "        im = cv2.imread(d[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "        scores = instances.scores if instances.has(\"scores\") else None\n",
    "        classes = instances.pred_classes if instances.has(\"pred_classes\") else None\n",
    "        \n",
    "        for i in range(len(instances)):\n",
    "            bbox = boxes[i].tensor.numpy().tolist()[0] if boxes is not None else []\n",
    "            score = scores[i].item() if scores is not None else 0\n",
    "            class_id = classes[i].item() if classes is not None else -1\n",
    "            class_name = metadata.thing_classes[class_id] if class_id != -1 else \"unknown\"\n",
    "            \n",
    "            # Write the prediction to the CSV file\n",
    "            writer.writerow({\n",
    "                'file_name': os.path.basename(d[\"file_name\"]),  # Save only the file name\n",
    "                'class_id': class_id,\n",
    "                'class_name': class_name,\n",
    "                'score': score,\n",
    "                'bbox': bbox\n",
    "            })\n",
    "\n",
    "        # Visualize the predictions and save the image with the original file name\n",
    "        v = Visualizer(im[:, :, ::-1], metadata, scale=1.2)\n",
    "        out = v.draw_instance_predictions(instances)\n",
    "        result_image_path = os.path.join(output_results_dir, os.path.basename(d[\"file_name\"]))\n",
    "        cv2.imwrite(result_image_path, out.get_image()[:, :, ::-1])\n",
    "        print(f\"Saved {result_image_path}\")\n",
    "\n",
    "print(f\"Predictions saved to {csv_file_path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Group by 'file_name' and 'class_name', and count occurrences\n",
    "counts_per_category = df.groupby(['file_name', 'class_name']).size().reset_index(name='count')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Pivot the table to have 'file_name' as rows, 'class_name' as columns, and 'count' as values\n",
    "counts_pivot = counts_per_category.pivot(index='file_name', columns='class_name', values='count').fillna(0)\n",
    "\n",
    "# Sort the pivoted counts DataFrame by 'file_name' alphabetically\n",
    "counts_pivot_sorted = counts_pivot.sort_index()\n",
    "\n",
    "# Convert all values in the DataFrame to integers\n",
    "counts_pivot_sorted = counts_pivot_sorted.astype(int)\n",
    "\n",
    "# Save the counts per category for each file name to a CSV file\n",
    "output_results_dir = \"net_data/output_results/predictions\"\n",
    "counts_pivot_csv_path = os.path.join(output_results_dir, \"counts_per_category_per_file.csv\")\n",
    "counts_pivot_sorted.to_csv(counts_pivot_csv_path)\n",
    "\n",
    "print(f\"Counts per category per file saved to {counts_pivot_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to: net_data/validation_true_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Function to count instances per category per file\n",
    "def count_instances_per_file(annotations, images):\n",
    "    instances_per_file = {}\n",
    "\n",
    "    for annotation in annotations:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id']\n",
    "\n",
    "        # Find the image file name corresponding to this image_id\n",
    "        image_filename = None\n",
    "        for image in images:\n",
    "            if image['id'] == image_id:\n",
    "                image_filename = image['file_name']\n",
    "                break\n",
    "\n",
    "        if image_filename is not None:\n",
    "            # Initialize the count for this image file if it's not already in the dictionary\n",
    "            if image_filename not in instances_per_file:\n",
    "                instances_per_file[image_filename] = {'file_name': image_filename, 'NETs': 0, 'non-net': 0}\n",
    "\n",
    "            # Count instances of each category in this image file\n",
    "            if category_id == 1:\n",
    "                instances_per_file[image_filename]['NETs'] += 1\n",
    "            elif category_id == 2:\n",
    "                instances_per_file[image_filename]['non-net'] += 1\n",
    "\n",
    "    return instances_per_file\n",
    "\n",
    "# Load the JSON file\n",
    "json_file = \"net_data/validation_dataset/val.json\"  # Replace with your JSON file path\n",
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract annotations and images from data\n",
    "annotations = data['annotations']\n",
    "images = data['images']\n",
    "\n",
    "# Count instances per category per file\n",
    "instances_per_file = count_instances_per_file(annotations, images)\n",
    "\n",
    "# Write results to a CSV file\n",
    "csv_file = \"net_data/validation_true_labels.csv\"  # Replace with desired output CSV file path\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(instances_per_file, orient='index')\n",
    "\n",
    "# Sort the DataFrame by 'file_name' column\n",
    "df.sort_values(by='file_name', inplace=True)\n",
    "\n",
    "# Ensure the columns are in the correct order\n",
    "df = df[['file_name', 'NETs', 'non-net']]\n",
    "\n",
    "# Write the sorted DataFrame to CSV with numerical values\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"CSV file saved to: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.0-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.detectron2-env/lib/python3.11/site-packages (from scikit-learn) (2.0.0)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.14.0-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.14.0 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 379 200 407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_ground_truth = pd.read_csv(\"/Users/dk/Desktop/complete_detectron_/net_data/validation_true_labels.csv\")\n",
    "df_predicted = pd.read_csv(\"net_data/output_results/predictions/counts_per_category_per_file.csv\")\n",
    "\n",
    "# Extract the counts for NETs and non-nets from the ground truth and predicted data\n",
    "true_net = df_ground_truth['NETs'].tolist()\n",
    "true_nonnet = df_ground_truth['non-net'].tolist()\n",
    "predicted_net = df_predicted['NETs'].tolist()\n",
    "predicted_nonnet = df_predicted['non-net'].tolist()\n",
    "\n",
    "\n",
    "# Aggregate the total counts of NETs and nonNETs across all samples\n",
    "total_actual_nets = sum(true_net)\n",
    "total_actual_nonnets = sum(true_nonnet)\n",
    "total_predicted_nets = sum(predicted_net )\n",
    "total_predicted_nonnets = sum(predicted_nonnet)\n",
    "\n",
    "print(total_actual_nets,total_actual_nonnets,total_predicted_nets,total_predicted_nonnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Non-net       0.93      0.89      0.91       427\n",
      "         Net       0.76      0.84      0.80       180\n",
      "\n",
      "    accuracy                           0.87       607\n",
      "   macro avg       0.85      0.87      0.85       607\n",
      "weighted avg       0.88      0.87      0.88       607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given data\n",
    "true_net = 152\n",
    "predicted_net = 200\n",
    "true_nonnet = 379\n",
    "predicted_nonnet = 407\n",
    "\n",
    "# Calculate False Positives (FP) and False Negatives (FN)\n",
    "fp = predicted_net - true_net\n",
    "fn = predicted_nonnet - true_nonnet\n",
    "\n",
    "\n",
    "# Ensure FN is non-negative (in practice, it should be zero or positive)\n",
    "fn = max(fn, 0)\n",
    "\n",
    "# Create arrays of true labels and predicted labels\n",
    "# Assumption: total instances = TP + TN + FP + FN\n",
    "true_labels = [1] * true_net + [0] * true_nonnet + [1] * fn + [0] * fp\n",
    "predicted_labels = [1] * true_net + [0] * true_nonnet + [0] * fn + [1] * fp\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-net', 'Net'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['Non-net', 'Net'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dk/Desktop/complete_detectron_/.detectron2-env/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved net_data/output_results/predictions_test/Ctrl_2_result.png\n",
      "Saved net_data/output_results/predictions_test/Ctrl_3_result.png\n",
      "Saved net_data/output_results/predictions_test/Ctrl_1_result.png\n",
      "Saved net_data/output_results/predictions_test/PMA_1_result.png\n",
      "Saved net_data/output_results/predictions_test/PMA_2_result.png\n",
      "Saved net_data/output_results/predictions_test/PMA_3_result.png\n",
      "Predictions saved to net_data/output_results/predictions_test/predictions.csv\n",
      "Counts per category per file saved to net_data/output_results/predictions_test/counts_per_category_per_file.csv\n",
      "Segmentation of all images completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import csv\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# Paths to configuration and model weights\n",
    "config_file_path = \"net_data/output_results/config.yaml\"\n",
    "model_weights_path = \"net_data/output_results/model_final.pth\"\n",
    "input_images_directory = \"net_data/test_dataset\"\n",
    "output_directory = \"net_data/output_results/predictions_test\"\n",
    "\n",
    "# Configuration setup\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(config_file_path)\n",
    "cfg.MODEL.WEIGHTS = model_weights_path\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set a custom testing threshold\n",
    "\n",
    "# Use CPU for prediction\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to load test dataset\n",
    "def load_my_test_dataset(dataset_path):\n",
    "    dataset_dicts = []\n",
    "    # Placeholder for dataset loading logic\n",
    "    # Add your dataset loading logic here\n",
    "    return dataset_dicts\n",
    "\n",
    "# Register the test dataset\n",
    "DatasetCatalog.register(\"dataset_test\", lambda: load_my_test_dataset(input_images_directory))\n",
    "MetadataCatalog.get(\"dataset_test\").set(thing_classes=[\"NETs\", \"nonnet\"])\n",
    "\n",
    "# Metadata for the test dataset\n",
    "metadata = MetadataCatalog.get(\"dataset_test\")\n",
    "\n",
    "# Prepare CSV file for saving predictions\n",
    "csv_file_path = os.path.join(output_directory, \"predictions.csv\")\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['file_name', 'class_id', 'class_name', 'score', 'bbox']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Loop over the images in the input folder\n",
    "    for image_filename in os.listdir(input_images_directory):\n",
    "        if image_filename.endswith(\".tif\"):\n",
    "            image_path = os.path.join(input_images_directory, image_filename)\n",
    "            im = cv2.imread(image_path)\n",
    "            outputs = predictor(im)\n",
    "            instances = outputs[\"instances\"].to(\"cpu\")\n",
    "            boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "            scores = instances.scores if instances.has(\"scores\") else None\n",
    "            classes = instances.pred_classes if instances.has(\"pred_classes\") else None\n",
    "\n",
    "            for i in range(len(instances)):\n",
    "                bbox = boxes[i].tensor.numpy().tolist()[0] if boxes is not None else []\n",
    "                score = scores[i].item() if scores is not None else 0\n",
    "                class_id = classes[i].item() if classes is not None else -1\n",
    "                class_name = metadata.thing_classes[class_id] if class_id != -1 else \"unknown\"\n",
    "\n",
    "                # Write the prediction to the CSV file\n",
    "                writer.writerow({\n",
    "                    'file_name': image_filename,  # Save only the file name\n",
    "                    'class_id': class_id,\n",
    "                    'class_name': class_name,\n",
    "                    'score': score,\n",
    "                    'bbox': bbox\n",
    "                })\n",
    "\n",
    "            # Visualize the predictions and save the image with the original file name\n",
    "            v = Visualizer(im[:, :, ::-1], metadata, scale=1.2)\n",
    "            out = v.draw_instance_predictions(instances)\n",
    "            result_image_path = os.path.join(output_directory, os.path.splitext(image_filename)[0] + \"_result.png\")\n",
    "            cv2.imwrite(result_image_path, out.get_image()[:, :, ::-1])\n",
    "            print(f\"Saved {result_image_path}\")\n",
    "\n",
    "print(f\"Predictions saved to {csv_file_path}\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Group by 'file_name' and 'class_name', and count occurrences\n",
    "counts_per_category = df.groupby(['file_name', 'class_name']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the table to have 'file_name' as rows, 'class_name' as columns, and 'count' as values\n",
    "counts_pivot = counts_per_category.pivot(index='file_name', columns='class_name', values='count').fillna(0)\n",
    "\n",
    "# Sort the pivoted counts DataFrame by 'file_name' alphabetically\n",
    "counts_pivot_sorted = counts_pivot.sort_index()\n",
    "\n",
    "# Convert all values in the DataFrame to integers\n",
    "counts_pivot_sorted = counts_pivot_sorted.astype(int)\n",
    "\n",
    "# Save the counts per category for each file name to a CSV file\n",
    "counts_pivot_csv_path = os.path.join(output_directory, \"counts_per_category_per_file.csv\")\n",
    "counts_pivot_sorted.to_csv(counts_pivot_csv_path)\n",
    "\n",
    "print(f\"Counts per category per file saved to {counts_pivot_csv_path}\")\n",
    "\n",
    "print(\"Segmentation of all images completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".detectron2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
