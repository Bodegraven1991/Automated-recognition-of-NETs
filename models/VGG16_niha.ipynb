{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def segment_and_enhance_nets(image):\n",
    "    # Convert image to HSV color space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define the color range for the fluorescent marker\n",
    "    lower_color = np.array([30, 150, 50])  # Adjust these values based on the color\n",
    "    upper_color = np.array([85, 255, 255])\n",
    "    \n",
    "    # Create a mask for the color\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "    \n",
    "    # Apply the mask to get the segmented NETs\n",
    "    segmented_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    # Convert the segmented image to grayscale\n",
    "    gray = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Enhance shapes using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    enhanced = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return segmented_image, enhanced\n",
    "\n",
    "# Example usage\n",
    "image = cv2.imread('data/unlabeled images/Ctr._1h_DFO-Zeitkinetik_2.jpg')\n",
    "segmented_image, enhanced_shape = segment_and_enhance_nets(image)\n",
    "\n",
    "# Display the original, segmented, and enhanced images\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Segmented NETs')\n",
    "plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Enhanced Shape')\n",
    "plt.imshow(enhanced_shape, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2: CNN Training\n",
    "\n",
    "Next, use the segmented and shape-enhanced images to train the CNN.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a function to create a combined model\n",
    "def create_combined_model(input_shape):\n",
    "    # Load the pre-trained VGG16 model for color features\n",
    "    base_model_color = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    color_output = base_model_color.output\n",
    "    color_output = GlobalAveragePooling2D()(color_output)\n",
    "    \n",
    "    # Separate input for shape-enhanced images\n",
    "    shape_input = Input(shape=input_shape)\n",
    "    base_model_shape = VGG16(weights='imagenet', include_top=False, input_tensor=shape_input)\n",
    "    shape_output = base_model_shape.output\n",
    "    shape_output = GlobalAveragePooling2D()(shape_output)\n",
    "    \n",
    "    # Concatenate color and shape outputs\n",
    "    combined = concatenate([color_output, shape_output])\n",
    "    \n",
    "    # Add custom layers on top of the combined outputs\n",
    "    x = Dense(1024, activation='relu')(combined)\n",
    "    predictions = Dense(1, activation='linear')(x)  # For counting, use a single output neuron\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=[base_model_color.input, shape_input], outputs=predictions)\n",
    "    \n",
    "    # Freeze the layers of the base models\n",
    "    for layer in base_model_color.layers:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model_shape.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_combined_model((224, 224, 3))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load and preprocess data\n",
    "# Assuming you have prepared directories for segmented and enhanced shape images\n",
    "train_generator_color = datagen.flow_from_directory(\n",
    "    'path_to_training_data/segmented',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'  # Use 'sparse' if labels are integers (counts)\n",
    ")\n",
    "\n",
    "train_generator_shape = datagen.flow_from_directory(\n",
    "    'path_to_training_data/enhanced_shape',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "validation_generator_color = datagen.flow_from_directory(\n",
    "    'path_to_validation_data/segmented',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "validation_generator_shape = datagen.flow_from_directory(\n",
    "    'path_to_validation_data/enhanced_shape',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=[train_generator_color, train_generator_shape],\n",
    "    epochs=50,\n",
    "    validation_data=([validation_generator_color, validation_generator_shape])\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('NET_counting_model_combined.h5')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
